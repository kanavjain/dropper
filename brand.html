<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Sensory-Integrated Audio Visualizer</title>
    <style>
        body, html {
            margin: 0;
            overflow: hidden;
            height: 100%;
            background-color: black;
            touch-action: none;
        }
        #startButton {
            position: absolute;
            top: 20px;
            left: 20px;
            padding: 12px 24px;
            font-size: 16px;
            cursor: pointer;
            z-index: 1;
        }
    </style>
</head>
<body>
    <canvas id="glCanvas"></canvas>
    <button id="startButton">Start Audio Visualization</button>

    <script>
        const canvas = document.getElementById('glCanvas');
        const gl = canvas.getContext('webgl');

        if (!gl) {
            alert('WebGL not supported in this browser.');
            throw new Error('WebGL not supported');
        }

        // Resize canvas to fit the window
        function resizeCanvas() {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
            gl.viewport(0, 0, gl.drawingBufferWidth, gl.drawingBufferHeight);
        }
        window.addEventListener('resize', resizeCanvas);
        resizeCanvas();

        // Vertex shader source code
        const vertexShaderSource = `
            attribute vec2 a_position;
            varying vec2 v_uv;
            void main() {
                v_uv = a_position * 0.5 + 0.5;
                gl_Position = vec4(a_position, 0.0, 1.0);
            }
        `;

        // Fragment shader source code
        const fragmentShaderSource = `
            precision highp float;
            varying vec2 v_uv;
            uniform float u_time;
            uniform vec2 u_resolution;
            uniform float u_bass;
            uniform float u_mid;
            uniform float u_treble;
            uniform vec2 u_mouse;
            uniform vec2 u_touch;
            uniform vec3 u_orientation;

            float random(vec2 st) {
                return fract(sin(dot(st.xy, vec2(12.9898,78.233))) * 43758.5453123);
            }

            float noise(vec2 st) {
                vec2 i = floor(st);
                vec2 f = fract(st);

                float a = random(i);
                float b = random(i + vec2(1.0, 0.0));
                float c = random(i + vec2(0.0, 1.0));
                float d = random(i + vec2(1.0, 1.0));

                vec2 u = f * f * (3.0 - 2.0 * f);

                return mix(a, b, u.x) + (c - a) * u.y * (1.0 - u.x) + (d - b) * u.x * u.y;
            }

            void main() {
                vec2 uv = v_uv;
                vec2 centeredUv = uv - 0.5;

                float time = u_time * 0.5;

                // Apply device orientation to distort the UV coordinates
                centeredUv += u_orientation.xy * 0.05;

                // Generate noise patterns
                float n = noise(centeredUv * 5.0 + time);
                float n2 = noise(centeredUv * 10.0 - time);

                // Create dynamic color patterns based on audio frequencies
                float bassEffect = u_bass * 2.0;
                float midEffect = u_mid * 2.0;
                float trebleEffect = u_treble * 2.0;

                vec3 color = vec3(0.0);

                // Apply bass effect
                color.r += smoothstep(0.4, 0.6, n + bassEffect * 0.5);

                // Apply mid effect
                color.g += smoothstep(0.3, 0.7, n2 + midEffect * 0.5);

                // Apply treble effect
                color.b += smoothstep(0.2, 0.8, n + trebleEffect * 0.5);

                // Add radial gradient
                float radius = length(centeredUv) * 2.0;
                color *= smoothstep(1.0, 0.0, radius);

                // Add mouse interaction effect
                float mouseDist = length(centeredUv - (u_mouse - 0.5));
                color += vec3(0.2 / (mouseDist + 0.1));

                // Add touch interaction effect
                float touchDist = length(centeredUv - (u_touch - 0.5));
                color += vec3(0.2 / (touchDist + 0.1));

                gl_FragColor = vec4(color, 1.0);
            }
        `;

        // Compile shader and create program
        function createShader(gl, type, source) {
            const shader = gl.createShader(type);
            gl.shaderSource(shader, source);
            gl.compileShader(shader);

            if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
                console.error('Shader compile failed:', gl.getShaderInfoLog(shader));
                gl.deleteShader(shader);
                throw new Error('Shader compile failed');
            }
            return shader;
        }

        const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
        const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);

        // Create and link the program
        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);

        if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
            console.error('Program link failed:', gl.getProgramInfoLog(program));
            gl.deleteProgram(program);
            throw new Error('Program link failed');
        }

        gl.useProgram(program);

        // Set up geometry data (full-screen quad)
        const positionBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        const positions = [
            -1, -1, // Bottom left
             1, -1, // Bottom right
            -1,  1, // Top left
            -1,  1, // Top left
             1, -1, // Bottom right
             1,  1, // Top right
        ];
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);

        const positionLocation = gl.getAttribLocation(program, 'a_position');
        gl.enableVertexAttribArray(positionLocation);
        gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 0, 0);

        // Get uniform locations
        const timeLocation = gl.getUniformLocation(program, 'u_time');
        const resolutionLocation = gl.getUniformLocation(program, 'u_resolution');
        const bassLocation = gl.getUniformLocation(program, 'u_bass');
        const midLocation = gl.getUniformLocation(program, 'u_mid');
        const trebleLocation = gl.getUniformLocation(program, 'u_treble');
        const mouseLocation = gl.getUniformLocation(program, 'u_mouse');
        const touchLocation = gl.getUniformLocation(program, 'u_touch');
        const orientationLocation = gl.getUniformLocation(program, 'u_orientation');

        // Set static uniforms
        gl.uniform2f(resolutionLocation, canvas.width, canvas.height);

        // Initialize variables
        let startTime = null;
        let bass = 0.0;
        let mid = 0.0;
        let treble = 0.0;
        let mousePosition = [0.5, 0.5];
        let touchPosition = [0.5, 0.5];
        let audioAnalyser = null;
        let lastVibrateTime = 0;
        let orientation = [0.0, 0.0, 0.0];

        // Audio setup
        function setupAudio() {
            navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                audioAnalyser = audioContext.createAnalyser();
                audioAnalyser.fftSize = 256;
                source.connect(audioAnalyser);
            }).catch(err => {
                alert('Microphone access denied.');
                console.error('Error accessing microphone:', err);
            });
        }

        // Start audio on button click
        const startButton = document.getElementById('startButton');
        startButton.addEventListener('click', () => {
            setupAudio();
            startButton.style.display = 'none';
        });

        // Handle mouse movement
        canvas.addEventListener('mousemove', event => {
            const rect = canvas.getBoundingClientRect();
            mousePosition = [
                (event.clientX - rect.left) / canvas.width,
                1.0 - (event.clientY - rect.top) / canvas.height,
            ];
        });

        // Handle touch movement
        canvas.addEventListener('touchmove', event => {
            event.preventDefault();
            const touch = event.touches[0];
            const rect = canvas.getBoundingClientRect();
            touchPosition = [
                (touch.clientX - rect.left) / canvas.width,
                1.0 - (touch.clientY - rect.top) / canvas.height,
            ];
        }, { passive: false });

        // Handle device orientation
        window.addEventListener('deviceorientation', event => {
            const gamma = event.gamma || 0; // Left to right tilt in degrees [-90,90]
            const beta = event.beta || 0;   // Front to back tilt in degrees [-180,180]
            const alpha = event.alpha || 0; // Compass direction

            // Normalize the orientation values
            orientation = [
                gamma / 90.0,  // X-axis
                beta / 180.0,  // Y-axis
                alpha / 360.0  // Z-axis
            ];
        });

        // Render loop
        function render(currentTime) {
            if (!startTime) startTime = currentTime;
            const elapsedTime = (currentTime - startTime) / 1000.0;

            // Update uniforms
            gl.uniform1f(timeLocation, elapsedTime);
            gl.uniform2f(mouseLocation, mousePosition[0], mousePosition[1]);
            gl.uniform2f(touchLocation, touchPosition[0], touchPosition[1]);
            gl.uniform3f(orientationLocation, orientation[0], orientation[1], orientation[2]);

            if (audioAnalyser) {
                const bufferLength = audioAnalyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                audioAnalyser.getByteFrequencyData(dataArray);

                // Divide frequency data into bass, mid, and treble
                const bassData = dataArray.slice(0, bufferLength / 4);
                const midData = dataArray.slice(bufferLength / 4, bufferLength / 2);
                const trebleData = dataArray.slice(bufferLength / 2);

                // Calculate average values
                const bassAvg = bassData.reduce((a, b) => a + b, 0) / bassData.length / 255.0;
                const midAvg = midData.reduce((a, b) => a + b, 0) / midData.length / 255.0;
                const trebleAvg = trebleData.reduce((a, b) => a + b, 0) / trebleData.length / 255.0;

                // Smooth the values
                bass += (bassAvg - bass) * 0.1;
                mid += (midAvg - mid) * 0.1;
                treble += (trebleAvg - treble) * 0.1;

                gl.uniform1f(bassLocation, bass);
                gl.uniform1f(midLocation, mid);
                gl.uniform1f(trebleLocation, treble);

                // Haptic feedback based on bass levels
                const currentTimeMs = performance.now();
                if (bass > 0.5 && currentTimeMs - lastVibrateTime > 500) {
                    if (navigator.vibrate) {
                        navigator.vibrate(100);
                    }
                    lastVibrateTime = currentTimeMs;
                }
            }

            // Clear and draw
            gl.clear(gl.COLOR_BUFFER_BIT);
            gl.drawArrays(gl.TRIANGLES, 0, 6);

            requestAnimationFrame(render);
        }
        requestAnimationFrame(render);
    </script>
</body>
</html>
