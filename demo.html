<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Music Reactive Visuals - Enhanced with WebGL</title>
  <style>
    body, html {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: black;
      touch-action: none;
      color: white;
      font-family: Arial, sans-serif;
    }

    canvas {
      display: block;
      width: 100%;
      height: 100%;
    }

    #info {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 10;
    }
  </style>
</head>
<body>

<div id="info">
  <div id="decibelMeter">Decibels: 0 dB</div>
  <div id="bpm">BPM: Calculating...</div>
  <div id="key">Key: Detecting...</div>
</div>
<canvas id="visualizer"></canvas>

<script>
  const canvas = document.getElementById('visualizer');
  const gl = canvas.getContext('webgl');
  const decibelMeter = document.getElementById('decibelMeter');
  const bpmDisplay = document.getElementById('bpm');
  const keyDisplay = document.getElementById('key');
  let audioContext, analyser, microphone;
  let bufferLength, dataArray;
  let particles = [];
  let ripples = [];
  let lastPeakTime = 0;
  let peaks = [];
  let key = 'Unknown';

  // WebGL shader setup
  const vertexShaderSource = `
    attribute vec2 a_position;
    void main() {
      gl_Position = vec4(a_position, 0, 1);
    }
  `;

  const fragmentShaderSource = `
    precision mediump float;
    uniform float u_time;
    uniform vec2 u_resolution;
    uniform sampler2D u_texture;
    void main() {
      vec2 uv = gl_FragCoord.xy / u_resolution;
      vec3 color = 0.5 + 0.5 * cos(u_time + uv.xyx + vec3(0, 2, 4));
      gl_FragColor = vec4(color, 1.0);
    }
  `;

  function createShader(gl, type, source) {
    const shader = gl.createShader(type);
    gl.shaderSource(shader, source);
    gl.compileShader(shader);
    if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
      console.error('Error compiling shader:', gl.getShaderInfoLog(shader));
      gl.deleteShader(shader);
      return null;
    }
    return shader;
  }

  function createProgram(gl, vertexShader, fragmentShader) {
    const program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
      console.error('Error linking program:', gl.getProgramInfoLog(program));
      gl.deleteProgram(program);
      return null;
    }
    return program;
  }

  const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
  const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);
  const program = createProgram(gl, vertexShader, fragmentShader);

  const positionAttributeLocation = gl.getAttribLocation(program, 'a_position');
  const resolutionUniformLocation = gl.getUniformLocation(program, 'u_resolution');
  const timeUniformLocation = gl.getUniformLocation(program, 'u_time');

  const positionBuffer = gl.createBuffer();
  gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
  const positions = [
    -1, -1,
    1, -1,
    -1, 1,
    -1, 1,
    1, -1,
    1, 1,
  ];
  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);

  // Resizing the canvas
  function resizeCanvas() {
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;
    gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);
  }
  window.addEventListener('resize', resizeCanvas);
  resizeCanvas();

  // Automatically start microphone input
  async function startVisualizer() {
    try {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      analyser = audioContext.createAnalyser();
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      microphone = audioContext.createMediaStreamSource(stream);
      microphone.connect(analyser);
      analyser.fftSize = 2048;

      bufferLength = analyser.frequencyBinCount;
      dataArray = new Uint8Array(bufferLength);
      
      animate();
      calculateBPM();
    } catch (err) {
      console.error('Error accessing microphone:', err);
    }
  }

  startVisualizer();

  // Visualizer animation
  function animate(time) {
    requestAnimationFrame(animate);
    analyser.getByteFrequencyData(dataArray);

    // Calculate decibel level
    let sum = dataArray.reduce((a, b) => a + b, 0);
    let average = sum / dataArray.length;
    let decibels = 20 * Math.log10(average / 255);
    decibelMeter.innerText = `Decibels: ${Math.max(0, decibels.toFixed(1))} dB`;

    // WebGL rendering
    gl.clearColor(0, 0, 0, 1);
    gl.clear(gl.COLOR_BUFFER_BIT);

    gl.useProgram(program);

    gl.enableVertexAttribArray(positionAttributeLocation);
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
    gl.vertexAttribPointer(positionAttributeLocation, 2, gl.FLOAT, false, 0, 0);

    gl.uniform2f(resolutionUniformLocation, gl.canvas.width, gl.canvas.height);
    gl.uniform1f(timeUniformLocation, time * 0.001);

    gl.drawArrays(gl.TRIANGLES, 0, 6);
  }

  // BPM Calculation
  function calculateBPM() {
    setInterval(() => {
      let maxAmplitude = Math.max(...dataArray);
      if (maxAmplitude > 200) {
        let currentTime = audioContext.currentTime;
        if (currentTime - lastPeakTime > 0.2) {
          peaks.push(currentTime);
          lastPeakTime = currentTime;

          // Calculate BPM
          if (peaks.length > 1) {
            let intervals = peaks.slice(1).map((time, index) => time - peaks[index]);
            let avgInterval = intervals.reduce((a, b) => a + b, 0) / intervals.length;
            let bpm = Math.round(60 / avgInterval);
            bpmDisplay.innerText = `BPM: ${bpm}`;
          }
        }
      }
    }, 100);
  }
</script>

</body>
</html>
