<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Trippy Fluid Flow Visualizer for Concerts</title>
    <style>
        body, html {
            margin: 0;
            overflow: hidden;
            height: 100%;
            background-color: black;
        }
        canvas {
            display: block;
            width: 100vw;
            height: 100vh;
        }
    </style>
</head>
<body>
    <canvas id="glCanvas"></canvas>
    <script>
        const canvas = document.getElementById('glCanvas');
        const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');

        if (!gl) {
            console.error('WebGL not supported');
            throw new Error('WebGL not supported');
        }

        // Resize canvas
        function resizeCanvas() {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
            gl.viewport(0, 0, canvas.width, canvas.height);
        }
        window.addEventListener('resize', resizeCanvas);
        resizeCanvas();

        // Compile shader
        function compileShader(source, type) {
            const shader = gl.createShader(type);
            gl.shaderSource(shader, source);
            gl.compileShader(shader);
            if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
                console.error('Shader compile failed: ' + gl.getShaderInfoLog(shader));
                gl.deleteShader(shader);
                throw new Error('Shader compile error');
            }
            return shader;
        }

        // Vertex shader source
        const vertexShaderSource = `
            attribute vec2 a_position;
            void main() {
                gl_Position = vec4(a_position, 0.0, 1.0);
            }
        `;

        // Fragment shader source for enhanced fluid flow effect
        const fragmentShaderSource = `
            precision mediump float;
            uniform float u_time;
            uniform vec2 u_resolution;
            uniform float u_audio;
            uniform vec3 u_orientation;
            uniform vec2 u_touch;
            
            vec2 rotate(vec2 v, float a) {
                float s = sin(a);
                float c = cos(a);
                return vec2(c * v.x - s * v.y, s * v.x + c * v.y);
            }
            
            void main() {
                vec2 uv = gl_FragCoord.xy / u_resolution.xy;
                uv -= 0.5;
                uv.x *= u_resolution.x / u_resolution.y;
                
                float time = u_time * 0.2;
                vec2 flow = uv;
                
                // Apply orientation-based distortion
                flow.x += u_orientation.x * 0.2;
                flow.y += u_orientation.y * 0.2;

                // Apply touch interaction
                float touch_dist = length(flow - u_touch);
                flow += 0.1 / (touch_dist + 0.01) * vec2(sin(u_time), cos(u_time));
                
                // Add fluid-like distortion with audio reactivity
                for (int i = 0; i < 8; i++) {
                    flow = rotate(flow, sin(time + length(flow) * 5.0 + float(i) * 0.5) * 0.5);
                    flow += vec2(sin(flow.y * 10.0 + time + u_audio * 3.0), cos(flow.x * 10.0 + time + u_audio * 2.0)) * 0.05;
                }
                
                float metallic = abs(sin(length(flow) * 10.0 + u_audio * 5.0 - time)) * 0.8 + 0.2;
                vec3 color = vec3(metallic * 0.6, 0.3 + 0.7 * metallic, 1.0 - metallic);
                
                gl_FragColor = vec4(color, 1.0);
            }
        `;

        // Create and link program
        const vertexShader = compileShader(vertexShaderSource, gl.VERTEX_SHADER);
        const fragmentShader = compileShader(fragmentShaderSource, gl.FRAGMENT_SHADER);
        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);

        if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
            console.error('Program link failed: ' + gl.getProgramInfoLog(program));
            throw new Error('Program link error');
        }
        gl.useProgram(program);

        // Set up geometry
        const positionBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([
            -1, -1,
            1, -1,
            -1, 1,
            -1, 1,
            1, -1,
            1, 1,
        ]), gl.STATIC_DRAW);

        const positionLocation = gl.getAttribLocation(program, 'a_position');
        gl.enableVertexAttribArray(positionLocation);
        gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 0, 0);

        // Set up uniforms
        const timeLocation = gl.getUniformLocation(program, 'u_time');
        const resolutionLocation = gl.getUniformLocation(program, 'u_resolution');
        const audioLocation = gl.getUniformLocation(program, 'u_audio');
        const orientationLocation = gl.getUniformLocation(program, 'u_orientation');
        const touchLocation = gl.getUniformLocation(program, 'u_touch');

        let audioLevel = 0.0;
        let orientation = { alpha: 0, beta: 0, gamma: 0 };
        let touch = [0.0, 0.0];

        // Set up microphone input
        navigator.mediaDevices.getUserMedia({ audio: true, video: false }).then(stream => {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function updateAudioData() {
                analyser.getByteFrequencyData(dataArray);
                audioLevel = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength / 256.0;
                requestAnimationFrame(updateAudioData);
            }
            updateAudioData();
        }).catch(err => {
            console.error('Error accessing microphone: ' + err);
        });

        // Set up device orientation
        window.addEventListener('deviceorientation', (event) => {
            orientation.alpha = event.alpha;
            orientation.beta = event.beta;
            orientation.gamma = event.gamma;
        });

        // Set up touch interaction
        canvas.addEventListener('touchmove', (event) => {
            const touchEvent = event.touches[0];
            touch = [
                (touchEvent.clientX / window.innerWidth) * 2.0 - 1.0,
                1.0 - (touchEvent.clientY / window.innerHeight) * 2.0
            ];
        });

        // Render loop
        function render(time) {
            time *= 0.001; // Convert time to seconds

            // Clear canvas
            gl.clearColor(0, 0, 0, 1);
            gl.clear(gl.COLOR_BUFFER_BIT);

            // Set uniforms
            gl.uniform1f(timeLocation, time);
            gl.uniform2f(resolutionLocation, canvas.width, canvas.height);
            gl.uniform1f(audioLocation, audioLevel);

            // Normalize orientation values and set uniform
            const orientationVec = [
                orientation.gamma / 90.0,  // X-axis tilt normalized to [-1, 1]
                orientation.beta / 90.0,   // Y-axis tilt normalized to [-1, 1]
                orientation.alpha / 360.0  // Z-axis rotation normalized to [0, 1]
            ];
            gl.uniform3fv(orientationLocation, orientationVec);

            // Set touch uniform
            gl.uniform2fv(touchLocation, touch);

            // Draw
            gl.drawArrays(gl.TRIANGLES, 0, 6);

            requestAnimationFrame(render);
        }
        requestAnimationFrame(render);
    </script>
</body>
</html>
