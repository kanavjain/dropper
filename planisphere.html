<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebGL Music Visualizer</title>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            background-color: black;
            touch-action: none;
        }
        canvas {
            display: block;
        }
        #startButton {
            position: absolute;
            top: 20px;
            left: 20px;
            z-index: 10;
            padding: 10px;
            background: rgba(0, 0, 0, 0.7);
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <canvas id="glCanvas"></canvas>
    <button id="startButton">Start Audio Visualization</button>
    <script>
        const canvas = document.getElementById("glCanvas");
        const gl = canvas.getContext("webgl");
        const startButton = document.getElementById("startButton");

        if (!gl) {
            alert("Unable to initialize WebGL. Your browser may not support it.");
            throw new Error("WebGL not supported");
        }

        function resizeCanvas() {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
            gl.viewport(0, 0, gl.drawingBufferWidth, gl.drawingBufferHeight);
        }
        window.addEventListener('resize', resizeCanvas);
        resizeCanvas();

        const vertexShaderSource = `
            attribute vec4 a_position;
            void main() {
                gl_Position = a_position;
            }
        `;

        const fragmentShaderSource = `
            precision highp float;
            uniform float u_time;
            uniform vec2 u_resolution;
            uniform float u_audioData;

            vec2 hash(vec2 p) {
                p = vec2(dot(p, vec2(127.1, 311.7)), dot(p, vec2(269.5, 183.3)));
                return -1.0 + 2.0 * fract(sin(p) * 43758.5453123);
            }

            float noise(vec2 p) {
                vec2 i = floor(p);
                vec2 f = fract(p);
                vec2 u = f * f * (3.0 - 2.0 * f);
                return mix(mix(dot(hash(i + vec2(0.0, 0.0)), f - vec2(0.0, 0.0)),
                               dot(hash(i + vec2(1.0, 0.0)), f - vec2(1.0, 0.0)), u.x),
                           mix(dot(hash(i + vec2(0.0, 1.0)), f - vec2(0.0, 1.0)),
                               dot(hash(i + vec2(1.0, 1.0)), f - vec2(1.0, 1.0)), u.x), u.y);
            }

            void main() {
                vec2 uv = gl_FragCoord.xy / u_resolution;
                uv -= 0.5;
                uv *= 2.0;
                uv.x *= u_resolution.x / u_resolution.y;

                float n = noise(uv * 3.0 + u_time * 0.2);
                float wave = sin(u_time * 0.5 + n * 3.0) * 0.1;
                uv.y += wave;

                float fluidFlow = noise(uv * 5.0 + u_time * 0.3) * 0.5;
                vec3 color = vec3(0.5 + 0.5 * sin(u_time + uv.xyx * 10.0 + u_audioData * 3.0));
                color = mix(color, vec3(0.2, 0.4, 0.8), 0.3); // Soothing blue tones
                color += fluidFlow * 0.3;

                gl_FragColor = vec4(color, 1.0);
            }
        `;

        function createShader(gl, type, source) {
            const shader = gl.createShader(type);
            gl.shaderSource(shader, source);
            gl.compileShader(shader);
            if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
                console.error(gl.getShaderInfoLog(shader));
                gl.deleteShader(shader);
                return null;
            }
            return shader;
        }

        const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
        const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);

        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
            console.error(gl.getProgramInfoLog(program));
            gl.deleteProgram(program);
            throw new Error("Program failed to link");
        }

        gl.useProgram(program);

        const positionBuffer = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
        const positions = [
            -1.0, -1.0,
             1.0, -1.0,
            -1.0,  1.0,
            -1.0,  1.0,
             1.0, -1.0,
             1.0,  1.0
        ];
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);

        const positionAttributeLocation = gl.getAttribLocation(program, "a_position");
        gl.enableVertexAttribArray(positionAttributeLocation);
        gl.vertexAttribPointer(positionAttributeLocation, 2, gl.FLOAT, false, 0, 0);

        const resolutionUniformLocation = gl.getUniformLocation(program, "u_resolution");
        const timeUniformLocation = gl.getUniformLocation(program, "u_time");
        const audioDataUniformLocation = gl.getUniformLocation(program, "u_audioData");

        gl.uniform2f(resolutionUniformLocation, canvas.width, canvas.height);

        let time = 0.0;
        let audioData = 0.0;
        let analyser;

        async function setupAudio() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const audioContext = new AudioContext();
                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                source.connect(analyser);
                startButton.style.display = 'none';
                requestAnimationFrame(render);
                getAudioData();
            } catch (err) {
                alert("Unable to access the microphone. Please allow microphone permissions and reload the page.");
                console.error("Error capturing audio: ", err);
            }
        }

        function getAudioData() {
            if (analyser) {
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
                audioData = average / 128.0;
                requestAnimationFrame(getAudioData);
            }
        }

        function render() {
            time += 0.01;
            gl.uniform1f(timeUniformLocation, time);
            gl.uniform1f(audioDataUniformLocation, audioData);

            gl.clear(gl.COLOR_BUFFER_BIT);
            gl.drawArrays(gl.TRIANGLES, 0, 6);

            requestAnimationFrame(render);
        }

        startButton.addEventListener('click', setupAudio);

        // Add touch interaction for a more soothing experience on smartphones
        let touchStart = null;
        let cameraOffset = [0, 0];

        canvas.addEventListener('touchstart', (event) => {
            if (event.touches.length === 1) {
                touchStart = { x: event.touches[0].clientX, y: event.touches[0].clientY };
            }
        });

        canvas.addEventListener('touchmove', (event) => {
            if (event.touches.length === 1 && touchStart) {
                const touchCurrent = { x: event.touches[0].clientX, y: event.touches[0].clientY };
                const deltaX = touchCurrent.x - touchStart.x;
                const deltaY = touchCurrent.y - touchStart.y;

                cameraOffset[0] += deltaX * 0.005; // Slow down for a more gentle experience
                cameraOffset[1] -= deltaY * 0.005;

                touchStart = touchCurrent;
            }
        });

        canvas.addEventListener('touchend', () => {
            touchStart = null;
        });

        function renderWithTouch() {
            time += 0.01;
            gl.uniform1f(timeUniformLocation, time);
            gl.uniform1f(audioDataUniformLocation, audioData);

            // Apply camera offset based on touch interaction
            gl.uniform2f(resolutionUniformLocation, canvas.width + cameraOffset[0], canvas.height + cameraOffset[1]);

            gl.clear(gl.COLOR_BUFFER_BIT);
            gl.drawArrays(gl.TRIANGLES, 0, 6);

            requestAnimationFrame(renderWithTouch);
        }

        requestAnimationFrame(renderWithTouch);
    </script>
</body>
</html>
