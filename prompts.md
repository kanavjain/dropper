### **1. Audio-Reactive Visualizer: Canvas Background**
- **Prompt**: "Create a WebGL visualizer that fills the canvas, reacting in real-time to low, mid, and high frequencies of microphone input. Use a mix of sinusoids and color blending to create layered, dreamy visuals. Adjust the background colors smoothly based on frequency input, aiming for a visually immersive and sensory-calming effect. Prioritize real-time performance."

### **2. Dynamic Stick Figure: SVG Layer**
- **Prompt**: "Generate a stick figure using SVG, with a central emoji head placed in front of the stick neck. The figure should take up most of the screen, staying centered as it floats around. Make sure the joints are realistically attached and update dynamically based on the audio frequencies. Link each limb (upper and lower arm, leg) to different audio bands: low to legs, mid to torso, high to arms. Animate the figure to make it look like it's dancing. Ensure the stick figure is never out of view."

### **3. Joint Movement Based on Audio Analysis**
- **Prompt**: "Map stick figure's movement to different sound aspects: arms move in response to high frequencies, legs to low frequencies, and torso to mid frequencies. Make sure that the movements follow a realistic pattern to give a lifelike dancing effect. Scale the movement's intensity based on frequency amplitude to adjust the dancing sensitivity. Keep each joint movement natural while ensuring the entire stickman remains centered within the canvas."

### **4. Floating Animation Logic**
- **Prompt**: "Ensure the stick figure floats smoothly around the screen, in sync with the music playing from the device. Movement should use sine and cosine functions to simulate gentle floating across the X and Y axes. Scale the movement based on the size of the screen to keep the figure always visible and well-proportioned."

### **5. Sensitivity and Tuning Controls**
- **Prompt**: "Add a start button for audio interaction and allow users to adjust sensitivity settings. Implement different levels of sensitivity for low, mid, and high frequencies to customize limb responsiveness to audio. Provide fallback options if microphone access is denied, including visual-only play mode with randomized movements to ensure inclusivity."

### **6. Layered Background Visual Effects**
- **Prompt**: "Use WebGL to create a layered background visual that responds to microphone input. Define three separate layers: a low-frequency layer with a dominant, pulsating visual (e.g., large blurry circles), a mid-frequency layer with slightly smaller, detailed animations (e.g., swirling lines), and a high-frequency layer with fast-changing fine details (e.g., sharp spikes or flashing dots). Blend these layers seamlessly and adjust alpha values to maintain a balance between sensory stimulation and comfort."

### **7. Device Compatibility and Performance Optimization**
- **Prompt**: "Ensure the visualizer works well across various devices and screen sizes. Implement a method to detect WebGL compatibility and adjust rendering details for low-power devices. Add an error message if WebGL isn't available, directing users to a simpler SVG-only version. Include an adaptive frame rate and reduce particle effects to enhance performance where needed."

### **8. Procedural Movement and Customization Options**
- **Prompt**: "Add movement customization to control how the stick figure dances: include adjustable parameters for movement range, speed, and amplitude sensitivity. Provide options for randomizing movement to create unique dance patterns or manually set speed and frequency sensitivity sliders. Ensure users have a good degree of control over visual and interactive aspects to make the experience personal and fun."

### **9. Head Emoji and Expressions**
- **Prompt**: "Include an interactive emoji head for the stick figure. When clicked, the emoji should change to a random face, selected from a set of eight expressions. Ensure the emoji is always in front of the neck and properly scaled for visibility. Make the head movements match the figure’s overall dance, adding slight bobbing or rotating to simulate enthusiasm."

### **10. Adaptive Camera Movement**
- **Prompt**: "Implement adaptive camera movement that zooms in and out slightly based on audio amplitude. Use a gentle zoom effect to simulate a ‘breathing’ visual tied to the beat, enhancing the immersive experience. Ensure that the movement remains smooth and doesn’t disorient viewers."

### **11. Weirdcore-Inspired Visualizer: Fluid Patterns**
- **Prompt**: "Create a WebGL visualizer inspired by weirdcore aesthetics, featuring fluid, shifting patterns and colors. The visual should respond in real-time to audio frequencies, with smooth distortions and unpredictable movements. Use GLSL shaders for creating psychedelic visuals and random noise to enhance the unpredictability. Focus on creating an unsettling yet fascinating sensory experience."

### **12. Procedural Scenery Visualizer: Audio-Driven Environment**
- **Prompt**: "Develop a visual toy that procedurally generates scenery in response to audio input, inspired by the visuals of *Star Guitar*. Include buildings, tracks, trees, and modular elements that respond dynamically to the beats and rhythms of the music. Use Three.js to create a calming, rhythmic scene where elements move in sync with the audio, with dynamic fog and lighting effects for added immersion."

### **13. Fractal Geometry and Particle Effects**
- **Prompt**: "Design a WebGL visualizer that combines fractal geometry, particle effects, and a torus knot. The visuals should react to audio input, with color changes and dynamic scaling based on low, mid, and high frequencies. Use Three.js and TensorFlow.js to create a mesmerizing and highly interactive experience, with fractals and particles moving in tandem with the music."

### **14. Customizable Visualizer with Color Modes**
- **Prompt**: "Create a WebGL visual toy where users can adjust color modes and sensitivity to audio input. Allow users to pick from different color schemes and adjust the sensitivity for each frequency band. Ensure that the visuals are responsive to both microphone and device audio, providing a highly customizable and personal sensory experience."

### **15. Synesthetic Visualizer: Multi-Touch and Audio Interaction**
- **Prompt**: "Develop a synesthetic visualizer that responds to both microphone and device audio input, providing a kaleidoscopic effect. Include support for multi-touch interaction and device orientation, allowing users to manipulate the visuals by touching the screen or tilting their device. Use WebGL to create dynamic, colorful visuals that respond in real-time to audio and user inputs."

### **16. Stick Figure Sync with Audio**
- **Prompt**: "Create a playful SVG-based stick figure visual toy that interacts with audio input. The stick figure should dance to the beat, with different body parts (e.g., arms, legs, head) responding to different frequency bands. Implement smooth, dynamic color transitions and playful movements that align with the audio’s energy level, making the stick figure come alive in response to the sound."

### **17. Dreamy Spectrograph Visualizer**
- **Prompt**: "Design a dreamy spectrograph visualizer using WebGL and 2D Canvas. The visuals should blend audio-reactive spectrograph overlays with smooth gradients and ripples that react to music or sound input. Aim for an ethereal and relaxing sensory experience, where the visuals smoothly change in response to the amplitude and frequency of the audio."

### **18. Real-Time Beat Detection for Visualizer**
- **Prompt**: "Enhance beat detection in a WebGL visualizer to make visual responses more dynamic. Fine-tune the beat detection algorithm to provide interesting visual responses to changes in the audio signal, such as larger visual pulses, sudden changes in color, or particle bursts that align with the beat. Focus on making the visualizer feel more connected to the rhythm of the audio for a captivating experience."
