<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dreamy Visuals with Audio Responsiveness</title>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            background-color: black;
            touch-action: none;
        }
        canvas {
            display: block;
        }
        #controls {
            position: absolute;
            top: 10px;
            left: 10px;
            z-index: 10;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.8);
        }
        #controls input[type=range] {
            width: 150px;
            margin-bottom: 10px;
            transition: all 0.2s;
        }
        #controls label {
            color: #ffffff;
            font-size: 14px;
            font-weight: bold;
            display: block;
            margin-bottom: 5px;
        }
    </style>
</head>
<body>
<canvas id="glCanvas"></canvas>
<div id="controls">
    <button id="startAudio">Start Audio Interaction</button>
</div>
<script>
    const canvas = document.getElementById("glCanvas");
    const gl = canvas.getContext("webgl");

    if (!gl) {
        alert("Unable to initialize WebGL. Your browser may not support it.");
        throw new Error("WebGL not supported");
    }

    function resizeCanvas() {
        canvas.width = window.innerWidth;
        canvas.height = window.innerHeight;
        gl.viewport(0, 0, gl.drawingBufferWidth, gl.drawingBufferHeight);
    }
    window.addEventListener('resize', resizeCanvas);
    resizeCanvas();
    
    const vertexShaderSource = `
        attribute vec4 a_position;
        void main() {
            gl_Position = a_position;
        }
    `;
    
    const fragmentShaderSource = `
        precision highp float;
        uniform float u_time;
        uniform vec2 u_resolution;
        uniform float u_audioData;
        
        // Function to create smoother gradients
        vec3 smoothGradient(vec2 uv, float timeOffset) {
            vec3 color = vec3(0.5 + 0.5 * sin(uv.x * 6.0 + u_audioData * 5.0 + timeOffset),
                              0.5 + 0.5 * cos(uv.y * 6.0 + u_audioData * 5.0 + timeOffset),
                              0.5 + 0.5 * sin((uv.x + uv.y) * 3.0 + u_audioData * 5.0 + timeOffset));
            return color;
        }
        
        void main() {
            vec2 uv = gl_FragCoord.xy / u_resolution.xy;
            uv -= 0.5; // Center the coordinates
            uv *= 2.0; // Zoom out slightly
            
            // Smoother fluid motion based on audio and time
            vec3 color = smoothGradient(uv, u_time * 0.5) * u_audioData;
            
            // Add subtle color pulsing based on time and audio data
            color += vec3(0.5 + 0.5 * sin(u_time * 0.3 + u_audioData * 2.0), 
                          0.5 + 0.5 * cos(u_time * 0.4 + u_audioData * 3.0), 
                          0.5 + 0.5 * sin(u_time * 0.5 + u_audioData * 1.5));
            
            // Output the color
            gl_FragColor = vec4(color, 1.0);
        }
    `;
    
    function createShader(gl, type, source) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
            console.error(gl.getShaderInfoLog(shader));
            gl.deleteShader(shader);
            return null;
        }
        return shader;
    }
    
    const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
    const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);
    
    const program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
        console.error(gl.getProgramInfoLog(program));
        gl.deleteProgram(program);
        throw new Error("Program failed to link");
    }
    
    gl.useProgram(program);
    
    const positionBuffer = gl.createBuffer();
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
    const positions = [
        -1.0, -1.0,
         1.0, -1.0,
        -1.0,  1.0,
        -1.0,  1.0,
         1.0, -1.0,
         1.0,  1.0
    ];
    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);
    
    const positionAttributeLocation = gl.getAttribLocation(program, "a_position");
    gl.enableVertexAttribArray(positionAttributeLocation);
    gl.vertexAttribPointer(positionAttributeLocation, 2, gl.FLOAT, false, 0, 0);
    
    const resolutionUniformLocation = gl.getUniformLocation(program, "u_resolution");
    const timeUniformLocation = gl.getUniformLocation(program, "u_time");
    const audioDataUniformLocation = gl.getUniformLocation(program, "u_audioData");
    
    gl.uniform2f(resolutionUniformLocation, canvas.width, canvas.height);

    let time = 0.0;
    let audioData = 0.0;
    
    async function setupAudio() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            const audioContext = new AudioContext();
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            
            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            
            function getAudioData() {
                analyser.getByteFrequencyData(dataArray);
                let average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
                audioData = average / 128.0;  // Normalize the value
                requestAnimationFrame(getAudioData);
            }
            
            getAudioData();
        } catch (err) {
            console.error("Error capturing audio: ", err);
            alert("Audio input is not available.");
        }
    }
    
    document.getElementById("startAudio").addEventListener("click", () => {
        setupAudio();
    });

    // Device Orientation Interaction
    if (window.DeviceOrientationEvent) {
        window.addEventListener('deviceorientation', (event) => {
            const { alpha, beta, gamma } = event;
            audioData += (Math.abs(beta) + Math.abs(gamma)) * 0.001; // Use device tilt as an additional factor for color variation
        });
    }

    // Touch Vibration Feedback
    function vibrate(duration) {
        if (navigator.vibrate) {
            navigator.vibrate(duration);
        }
    }
    canvas.addEventListener('touchstart', () => vibrate(50));

    function render() {
        time += 0.01;
        gl.uniform1f(timeUniformLocation, time);
        gl.uniform1f(audioDataUniformLocation, audioData);
        
        // Draw the scene
        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.drawArrays(gl.TRIANGLES, 0, 6);

        requestAnimationFrame(render);
    }

    requestAnimationFrame(render);
</script>
</body>
</html>
